# Reaction to Dennett Article

- themes are important, especially with deep fake videos
- recall that there was a politician whose career was ruined since her instagram stories/posts were modified and made pornographic
- have scam calls where voices are mimicked and then used to scam people out of money
- that being said, while there is a large need for AI moderation, a lot of the article seems to be strong feelings and not much persuading
- info= outdated, how do we control spiralling algorithms? in the US, AI wasn't even covered in constitution (argument used for many protections), therefore find it believe federal laws of large regulation will happen in time

Daniel C. Dennett's "The Problem with Counterfeit People" provides a compelling call for Artificial Intelligence (AI) moderation, but contains outdated cases due to its publication in 2023. While reading this article, I found myself agreeing with many of the themes and cases, especially in light of stories currently in the news. However, I recalled a news story about a New York politician whose Instagram stories and posts were morphed with the help of AI into pornographic versions, thus ruining her political career. Additionally, the cases in the article reminded me of a warning my parents gave me about scam calls that record your voice. These callers then go on to use your voice (fed through AI) to call other contacts in your phone feigning a call for help or money. While these are cases from the top of my head, I strongly believe that the article's main idea of AI moderation can be strengthened by including more relevant case studies of concern, especially since AI has evolved since May 2023. 

AI started as a powerful tool for collating large amounts of knowledge and providing accessibility, but the tools are being sharpened to become weapons. While I agree that a moderation tool is required to mitigate the dangers of AI, the article's suggestion of watermarks seems like an inadequate in the face of the current AI problems. Regulation of money (where the parallel for a watermark was sourced) works because "pure" money originates from the government--a singular source. In comparison, AI tools seem to spawn by the day, each with their own origination, meaning that compliance will be more difficult to maintain. AI development has suprisingly low barriers to creation, meaning anyone can learn how to develop an AI tool of their own. (This stems from AI's integration with nearly every aspect of our online experience, from personal assistance to data analysis and more.) It is unfortunately natural, in my mind, that if AI is used for good, it is also being used for bad. 

The article provides a very interesting solution to AI proliferation and misuse: watermarking AI products. While this might have been a robust solution in early 2023, in light of AI's rapid development and abuse, I firmly believe more regulation is required. There will always be maleficent AI tools available, but the barriers to those must increase so that they're obsolete from main, above-table markets. The first solution that comes to mind is creating a barrier of access, where it's no longer easy to access or create an AI tool of high complexity. Refined AI tools would then be controlled by large tech conglomerations and developed more in a contained environment. Research or working independently with an AI tool would require a license, much like when one wants to drive a car.  
