# Reaction to Dennett's Article

Daniel C. Dennett's article, "The Problem with Counterfeit People", offers an interesting solution for artificial intelligence (AI) moderation, focusing on watermarking to mitigate AI abuses. However, since he wrote the article in 2023, AI technology has evolved rapidly, with numerous examples of misuse demonstrate the need for more robust and multifaceted solutions. While the suggestions in "The Problem with Counterfeit People" are a step in the right direction, ongoing AI-related problems reveal how watermarks might be insufficient in addressing the complexity and dangers of AI abuse; today, we need stronger solutions to deal with how AI is being misused. 

## AI: A Tool That Can Help or Hurt

Recent news stories illustrate the dual nature of AI, which can both enhance and harm our lives. While AI has become integrated with nearly all aspects of our daily lives, it has also become a tool for evil. In New York, a politician's career was ruined after her Instagram stories and posts were manipulated with AI into pornographic content. This incident underscores the dangers of AI-generated deepfakes, which can spread misinformation and derail careers. Similarly, in 2022, a deepfake of Ukrainian President Volodymyr Zelensky urged Ukrainian soldiers to ceasefire against Russia, an attempt to manipulate moral during conflict. Although this video was debunked, it demonstrated the serious risks of deepfake technology being used for malicious purposes.

These examples show how easily AI can be weaponized, amplifying the harm caused by misinformation and manipulation. Despite AI's potential for positive impact, its darker side is becoming increasingly evident as it is used to deceive, harass, and exploit.

## Limitations of Dennett’s Watermarking Proposal

Dennett’s proposal of watermarking AI products, while well-intentioned, falls short in addressing the full scope of AI’s misuse. Watermarking works well for currency because it originates from a singular source: the government. However, AI tools are developed by countless entities, making it difficult to enforce universal compliance. As AI development requires relatively low technical barriers, anyone with enough knowledge can create powerful AI tools, rendering it nearly impossible to regulate effectively using watermarks alone.

In my opinion, AI moderation must go beyond watermarking and include measures that limit access to AI technology. The ease with which AI tools are currently developed and distributed raises concerns about their potential misuse. If we continue to allow unfettered access to sophisticated AI tools, the likelihood of harmful applications will only increase.

## The Need for Stronger Regulation: A Case Study

In light of the inadequacies of watermarking, more stringent regulation is required to control the proliferation of AI misuse. Government intervention, akin to the Federal Bureau of Investigation's crackdown on book pirating websites like Z-Library, could help restrict access to platforms offering AI tools for harmful purposes, such as deepfake or impersonation technologies. Such "clean-up" efforts could target platforms enabling malicious AI use, making it more difficult for bad actors to access these tools.

South Korea’s recent “deepfake crisis” provides a case study in the urgent need for stronger AI regulation. The country has seen a sharp increase in deepfake-related sexual harassment, with many cases involving teenagers and young adults. This has led to increased collaboration between law enforcement and government to address the crisis. Additionally, Telegram---a popular chat platform---is facing serious scrutiny for being a hub where many of these crimes occurred. South Korea’s proactive approach serves as a model for how other countries might tackle the growing dangers of AI.

## Current Regulations

Many international governments have made AI regulation a primary focus. The European Union proposed the AI Act in 2021, which classifies AI applications by risk: unacceptable risk, high risk, limited risk, and minimal risk. Regulations and penalties are assigned according to the corresponding risk level. The United States introduced the Blueprint for an AI Bill of Rights in October 2022, outlining guidelines for AI usage and systems. California notably passed several data privacy laws (e.g. the California Consumer Privacy Act) regulating AI and company practices. China has implemented many AI-regulating policies, such as the Regulations on the Management of Deep Synthesis (2023), Personal Information Protection Law, and the Algorithm Recommendation Management Rules. These steps, which are more stringent than watermarking, demonstrate the global push for AI management. 

## Conclusion

While Daniel C. Dennett's article offers a persuasive argument for AI moderation, it is now clear that watermarking alone is insufficient to combat the growing threats posed by AI. Governments around the world understand this and have passed multiple acts for AI regulation. The cases of deepfake manipulation and AI scams demonstrate the growing potential for harm, requiring more comprehensive regulatory solutions. Limiting access to AI tools and increasing government intervention could help mitigate these risks.

As South Korea's efforts demonstrate, current government initiatives require more attention to prevent further abuse. In light of these developments, I believe that Dennett’s argument would benefit from incorporating stronger measures to address the complexities of AI proliferation. AI is undoubtedly a powerful tool, and with careful regulation, it can be a powerful tool for good.



