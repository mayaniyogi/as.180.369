# Reaction to Dennett Article

<!-- - themes are important, especially with deep fake videos -->
<!-- - recall that there was a politician whose career was ruined since her instagram stories/posts were modified and made pornographic -->
<!-- - have scam calls where voices are mimicked and then used to scam people out of money -->
<!-- - that being said, while there is a large need for AI moderation, a lot of the article seems to be strong feelings and not much persuading -->
<!-- - info= outdated, how do we control spiralling algorithms? in the US, AI wasn't even covered in constitution (argument used for many % protections), therefore find it believe federal laws of large regulation will happen in time -->

Daniel C. Dennett's "The Problem with Counterfeit People" presents a compelling argument for artificial intelligence (AI) moderation, focusing on watermarking as a solution to mitigate AI abuses. However, AI technology has evolved rapidly since Dennett's publication in May 2023, with numerous examples of misuse demonstrate the need for more robust and multifaceted solutions. While the suggestions in "The Problem with Counterfeit People" are a step in the right direction, ongoing AI-related problems reveals how watermarks might be insufficient in addressing the complexity and dangers of AI abuse. 

## AI as a Double-Edged Sword

Recent news stories illustrate the dual nature of AI, which can both enhance and harm our lives. For example, in New York, a politician's career was ruined after her Instagram stories and posts were manipulated with AI into pornographic content. This incident underscores the dangers of AI-generated deepfakes, which can spread misinformation and derail careers. Similarly, in 2022, a deepfake of Ukrainian President Volodymyr Zelensky urged Ukrainian soldiers to ceasefire against Russia, an attempt to manipulate moral during conflict. Although this video was debunked, it demonstrated the serious risks of deepfake technology being used for malicious purposes.

These examples show how easily AI can be weaponized, amplifying the harm caused by misinformation and manipulation. Despite AI's potential for positive impact, its darker side is becoming increasingly evident as it is used to deceive, harass, and exploit.

## Limitations of Dennett’s Watermarking Proposal

Dennett’s proposal of watermarking AI products, while well-intentioned, falls short in addressing the full scope of AI’s misuse. Watermarking works well for currency because it originates from a singular source: the government. However, AI tools are developed by countless entities, making it difficult to enforce universal compliance. As AI development requires relatively low technical barriers, anyone with enough knowledge can create powerful AI tools, rendering it nearly impossible to regulate effectively using watermarks alone.

In my opinion, AI moderation must go beyond watermarking and include measures that limit access to AI technology. The ease with which AI tools are currently developed and distributed raises concerns about their potential misuse. If we continue to allow unfettered access to sophisticated AI tools, the likelihood of harmful applications will only increase.

## The Need for Stronger Regulation

In light of the inadequacies of watermarking, more stringent regulation is required to control the proliferation of AI misuse. Government intervention, akin to the Federal Bureau of Investigation's crackdown on book pirating websites like Z-Library, could help restrict access to platforms offering AI tools for harmful purposes, such as deepfake or impersonation technologies. Such "clean-up" efforts could target platforms enabling malicious AI use, making it more difficult for bad actors to access these tools.

South Korea’s recent “deepfake crisis” provides a case study in the urgent need for stronger AI regulation. The country has seen a sharp rise in deepfake-related sexual harassment, often targeting teenagers and young adults. This led to increased collaboration between law enforcement and government to address the crisis. Telegram, a popular chat platform, is facing serious scrutiny for being a hub where many of these crimes occurred. South Korea’s proactive approach serves as a model for how other countries might tackle the growing problem of AI misuse.

## Current Regulations
<!-- FLAG: need to edit this section more --> 

While AI abuse appears to be an inevitable future, many international governments have made AI regulation a primary focus. The European Union prosed the AI Act in 2021, which classified AI uses according to risk: unacceptable risk, high-rish, limited risk, and minimal risk. Regulations and penalties would be assigned according to corresponding risk level. The United States introduced the Blueprint for an AI Bill of Rights in October 2021, outlining guidelines for AI usage and systems. California notably passed seceral data privacy laws (e.g. the California Consumer Privacy Act) regulating AI and company usages. China has many AI-regulating policies, such as the Regulations on the Management of Deep Synthesis (2023), Personal Information Protection Law, and the Algorithm Recommendation Management Rules. These steps, more stringent than watermarking, demonstrate the global push for AI management. 

## Conclusion

While Daniel C. Dennett's article offers a persuasive argument for AI moderation, it is now clear that watermarking alone is insufficient to combat the rapidly evolving threats posed by AI. Global governments understand this and have passed multiple acts for AI regulation. The cases of deepfake manipulation and AI scams demonstrate the growing potential for harm, requiring more comprehensive regulatory solutions. Limiting access to AI tools and increasing government intervention could help mitigate these risks.

As South Korea's efforts demonstrate, current government initiatives require more attention to prevent further abuse. In light of these developments, I believe that Dennett’s argument would benefit from incorporating stronger measures to address the complexities of AI proliferation. AI is undoubtedly a powerful tool, but without careful regulation, it risks becoming a greater source of harm than good.



