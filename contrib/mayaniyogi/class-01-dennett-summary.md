# Reaction to Dennett Article

<!-- - themes are important, especially with deep fake videos -->
<!-- - recall that there was a politician whose career was ruined since her instagram stories/posts were modified and made pornographic -->
<!-- - have scam calls where voices are mimicked and then used to scam people out of money -->
<!-- - that being said, while there is a large need for AI moderation, a lot of the article seems to be strong feelings and not much persuading -->
<!-- - info= outdated, how do we control spiralling algorithms? in the US, AI wasn't even covered in constitution (argument used for many % protections), therefore find it believe federal laws of large regulation will happen in time -->

Daniel C. Dennett's "The Problem with Counterfeit People" presents a compelling argument for artificial intelligence (AI) moderation. News  from the last few years highlights the rapid evolution and misuse of AI technology. Dennett proposes watermarking as a solution for AI abuse, however, since Dennett's publication in May 2023, the rapid technological advances of AI necessitate more robust solutions than Dennett's strategy. 

<!-- AI started as a powerful tool for collating large amounts of knowledge and providing accessibility, but the tools are being sharpened to become weapons. In addition to the -->

While reading this article, I found myself agreeing with many of the themes and cases, especially in light of stories in the news. I recalled a news story about a New York politician whose Instagram stories and posts were morphed with the help of AI into pornographic versions, thus ruining her political career. More specically, in 2022, a deepfake of Ukrainian President Volodymyr Zelensky urging Ukrainian soldiers to ceasefire against Russia. Ultimately, the goal of this video was to reduce Ukrainian morale and hinder the Ukrainian effort. However, this video--while ultimately proven untrue--shows the harm deepfake technology can inflict. This video was the exception, not the norm, when it comes to deepfake debunking. These cases highlight how AI has become a double-edged sword: as it rapidly improves our day-to-day lives, it's also become better at harming others.


AI moderation must be more than a simple watermark; I believe that AI moderation begins by limiting the amount of usage. While I agree that a moderation tool is required to mitigate the dangers of AI, the article's suggestion of watermarks seems inadequate in the face of the current AI problems. Regulation of money (where the parallel for a watermark was sourced) works because "pure" money originates from the government--a singular source. In comparison, AI tools seem to spawn by the day, each with their own origination, meaning that compliance will be more difficult to maintain. AI development has surprisingly low barriers to creation, meaning anyone can learn how to develop an AI tool of their own. (This stems from AI's integration with nearly every aspect of our online experience, from personal assistance to data analysis and more.) It is unfortunately natural--in my mind--that if AI is used for good, it will inevitably be used for bad.

The article provides a very interesting solution to AI proliferation and misuse: watermarking AI products. While this might have been a robust solution in early 2023, in light of AI's rapid development and abuse, I firmly believe more regulation is required. There will always be maleficent AI tools available, but the barriers to those must increase so that they're obsolete from main, above-the-table markets. The first solution that comes to mind is creating a barrier of access, where it's no longer easy to access or create an AI tool of high complexity. I'm reminded of the Federal Bureau of Investigation actively seizing book pirating websites such as z-library for criminal copyright infringement. Similar to this "clean-up style," government agencies could restrict access to platforms that offer access to deepfake or impersonation tools.

Reading this article coincided with the news breaking about South Korea's "deepfake crisis," where their law enforcement and government are now working together to control deepfake technology abuse. This is due to the serious uptick in deepfake-related sexual harassment cases filed within the country--many of which targeted teenagers and young adults. Telegram--a popular and free chatting platform--is now also facing serious allegations due to many of these crimes taking place thereupon. As such, it is obvious that AI as a tool for evil has only grown worse, and serious regulation is required, more than what Dennett proposes. However, I am certain that Dennett's watermark could be easily circumvented; I am unsure of how adequate regulation could be structured in a proactive way, but I hope South Korea shows the world what must be done.

In conclusion, I agree with the sentiments from Daniel C. Dennett's "The Problem with Counterfeit People", but I believe that the argument could be strengthened in light of more recent AI-assisted abuses, especially those assisted via deepfake technology. AI is a tool that can both help and harm us, and I believe that more regulation must exist to safely utilize it. This regulation could also include creating barriers to entry in addition to the watermarking idea that Dennett touches upon in his article. His article is very persuasive from an early-2023 perspective, but our AI-related problems today have outpaced his 2023-based solutions. On a positive note, South Korea's proactive response to these crimes is revolutionary, creating hope that this misuse is only temporary.




